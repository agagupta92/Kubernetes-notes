What is containerization?
--Containers have become one of the most popular concepts used in IT and software industry in the last five years. Since the introduction of Docker, containers have evolved to a larger ecosystem that include many tools and technologies, including Docker and Kubernetes. Before we go into too much detail, let's look at why containers have become so popular and how they're used. First off, containers aren't a new topic. They've existed for a number of years and have taken many forms before the creation of Docker. As you can see here, in general a container is defined as a collection of software processes unified by one namespace with access to an operating system kernel that it shares with other containers and little or no access between them. Docker modifies this definition by saying that a container is a runtime instance of Docker images that contain three things, a Docker image, an execution environment, and a standard set of instructions. For those coming from an object oriented world, you can use the analogy of classes and objects, where a container is an object and the class is a Docker image. While Docker has many products and solutions, the core pieces of the ecosystem are the Docker Engine and the Docker Store, sometimes referred to as the Docker Hub. The Docker Engine is comprised of runtime and packaging tools and is required to be installed on the hosts that run Docker. The Docker Store is an online cloud service where users can store and share their Docker images. Docker provides many more services than we can cover here, but feel free to check out the Docker courses in the library. One question that comes up a lot is what is the difference between a container and a virtual machine? Containers might look like a VM, but these are two distinct technologies. In a VM, each virtual machine includes many applications, all the necessary binaries and libraries that would exist on the OS, and the entire guest operating system to interact with them. On the other hand, a container will include the application and all of its dependencies, but will share the kernel with the other containers. It is not tied to any specific infrastructure other than having the Docker Engine installed on its host. It'll run an isolated process in the user space on the host operating system. This allows containers to run on almost any computer, infrastructure, or cloud. From a high level, containers provide benefits to both developers and DevOps folk alike. Developers like them because it's easy to create applications that are portable and packaged in a standard way. They also make the process of deployment very easy and repeatable. Testing, packaging, and integrations can be automated in an easier way than before. Containers support newer microservices architectures, which fit better from a developer mindset. And finally, containers help alleviate platform compatibility issues. From a DevOps standpoint, using containers simplifies release management. Deployments become much more reliable, which improves the speed and frequency of releases. The application lifecycle is consistent. They can be configured once and run multiple times, making the process more repeatable and efficient. Environments can be made more consistent. No more process differences between the dev, staging, and production environments. Scaling applications also becomes a lot simpler. Containers take a few seconds to deploy to a host, which makes the process of adding extra workers easier and the workload can grow and shrink more quickly for on-demand use cases. One of the biggest value adds of using container technologies in an enterprise is that developers and DevOps team now have a common language to collaborate. Both sets of teams can describe their needs and architectures in terms of containers using the same vocabulary for dev and deployment. Issues that come up in production by a DevOps team can be easily communicated back to a development team. The dev team can isolate and debug specific issues to a container level, eliminating problems relating to differences in hosts or runtime issues with applications. With all the benefits that containers bring to the table, it's no wonder that there's such a dramatic increase in their use. A Forrester survey conducted in 2017 indicated that organizations expect the number of containerized apps will rise by 80% in 2019. The white paper is a great read and talks about some of the most prominent use cases of containers in different sized orgs. Organizations use containers to build applications to incorporate a microservices-based architecture. Newer applications are built with a microservices mindset using containers underneath to realize this, and legacy applications are shipped as containers to fit the microservices mold as well. Finally, containers assist with code agility and help you build a continuous integration or continuous deployment pipeline. This use case really pushes an IT team to develop, test, and deploy applications faster in a more automated fashion. Hopefully by now it's clear why enterprises have started to adopt containers in such a big way. Now that we understand what containers are all about, let's see where Kubernetes plays a role.

What is Kubernetes?
Selecting transcript lines in this section will navigate to timestamp in the video
- [Narrator] Now let's talk about Kubernetes, the most popular open-source container orchestrator available today. The adoption of Docker has really taken off in the last few years. Per the 2017 Docker survey by Datadog, Docker adoption was up 40% in Datadog's very large customer base. Additionally, the 2017 Docker Usage Report, conducted by Sysdig, stated that the median number of containers running on a single host is about 10. All this data begs an important question. How do you manage all these running containers on a single host, and more importantly, across your whole infrastructure? This is where the idea of container orchestrators come in. Container orchestration solves the problem of deploying multiple containers either by themselves or as a part of an application across many hosts. From a high level some of the features required are the ability to provision hosts, start containers in a host, be able to restart failing containers, have the ability to link containers together so that they can communicate with their peers, expose required containers as services to the world outside the cluster, and scaling the cluster up or down. There are a few solutions in the container orchestrations base, and we'll cover more of these later on. Kubernetes is an open-source platform designed to automate the deployment, scaling, and operation of containers. The real goal of the platform is to foster an ecosystem of components and tools that relieve the burden of running applications in public and private clouds. Kubernetes, often called K8S or Hubernetes, is an open-source platform that started at Google. Internally, all of the Google infrastructure relies on containers and generates more than two billion container deployments a week, all powered by an internal platform called Borg. Borg was the predecessor to Kubernetes and the lessons learned from developing Borg over the years has become the primary building blocks in the development of Kubernetes. Simply put, using Kubernetes in your infrastructure gives you a platform to schedule and run containers on clusters of your machines, whether it's on bare metal, virtual machines, in a private data center, or in the cloud. This means no more golden handcuffs and opens up opportunities to have hybrid cloud scenarios for your folks migrating towards the cloud. Because Kubernetes is a container platform, you can use Docker containers to develop and build applications, and then use Kubernetes to run these applications in your infrastructure. It's important to note that you don't have to use Docker containers. You can use rkt by CoreOS or other container platforms. However, most users in the container ecosystem use Docker containers, so we'll use Docker for this course. There are many companies that use Kubernetes today for different use cases. I'll discuss a few of these to document common ways companies are using Kubernetes. Box has been moving towards a MIC services architecture for their infrastructure and became an early adopter. Kubernetes has provided Box's developers a common development platform to build MIC services. Also, since Kubernetes can run on bare metal, just as well as the cloud, Box could create a migration strategy to Google Cloud. It could also use the same tools and concepts to run across their existing data center and the new Google Cloud. Pokemon GO. When a game has the motto "Got to catch them all," it retrospectively comes as no shock that as soon as it launched, within half an hour, traffic surged and crashed all Pokemon GO servers. To keep up with traffic, Niantic got help from teams at Google to move Pokemon GO to the Google container engine. To date, Pokemon GO is the largest Kubernetes deployment on the Google container engine. Due to the scale of the cluster and accompanying TrueBit, a multitude of bugs were identified, fixed, and merged back to the Kubernetes code base. eBay was an early adopter of OpenStack, and used OpenStack as the platform to run their virtual machines. However, as the infrastructure grew, developers found it easier to manage and deploy containers as opposed to working with .bms or applications directly on OpenStack. As a result, eBay moved to use the Kubernetes platform from OpenStack Magnum. This allowed the company to manage the infrastructure more easily and allowed their developers to continue to be agile and productive. In the next section, we'll take a look at why Kubernetes is so effective and why people like it so much.


Kubernetes features
-- To kick off this section, I wanted to give you my favorite definition of Kubernetes that I've heard. It's by Joe Beda, who's one of the original Kubernetes developers and has done some really cool things in the Kubernetes and cloud native landscape. He said, "Kubernetes is an open source project "that enables software teams of all sizes "from a small startup to a Fortune 100 company "to automate deploying, scaling and managing applications "on a group or cluster of server machines. "These applications can include everything "from internal-facing web applications, "like a content management system to marquee web properties "like Gmail to big data processing." I really like this quote because I think it encompasses everything when I think about Kubernetes. In this section, we'll break down the different features of Kubernetes that make it a great platform to manage your containerized applications. Let's take a look at some of these. First up, multi-host container scheduling. This feature is handled by the kube-scheduler which assigns containers, also known as pods in Kubernetes to nodes at runtime. It accounts for resources, quality of service, policies and user specifications before scheduling. Scalability and availability. The Kubernetes master can be deployed in a highly available configuration. Multi-region deployments are also available as well. From a scalability perspective, in Kubernetes 1.17, the architecture supports up to a 5000-node cluster and can run up to a maximum of a hundred and fifty thousand pods. The pods can also be horizontally scaled via an API. Flexibility and modularization. Kubernetes has a plug-and-play architecture that allows you to extend it when you need to. There are specific add-ons from network drivers, service discovery, container runtime, visualization and command. If there are tasks that you need to perform for your environment specifically, you can create an add-on to suit your needs. Two features that allow Kubernetes clusters to scale are registration and discovery. New worker nodes can seamlessly register themselves with the Kubernetes master node. Kubernetes also includes service discovery out of the box. Service discovery allows for automatic detection of new services and endpoints via DNS or environment variables. It also allows for persistent storage, which is a much requested feature when working with containers. Pods can use persistent volumes to store data and the data is retained across pod restarts and crashes. Application upgrades is one area where the Kubernetes project has done a lot of pioneering work. Application upgrades are supported out of the box as well as rollbacks. When it comes to Kubernetes maintenance and upgrades, Kubernetes features are always backward compatible for a few versions. All API's are versioned and when upgrading or running maintenance on the host, you can unschedule the host so that no deployments can take place on it. Once you're done, you can simply turn the host back on and schedule deployments or jobs. In terms of logging and monitoring, application monitoring or health checks are also built-in, TCP, HTTP or container exec health checks are available out of the box. There are also health checks to give you the status of the nodes and failure monitored by the node controller. Kubernetes status can also be monitored via add-ons like Metrics Server, cAdvisor and Prometheus. And lastly, you can use the built-in logging frameworks or if you choose, you can bring your own. Management of secrets. Sensitive data is a first-class citizen in Kubernetes. Secrets amounted as data volumes or environment variables. They are also specific to a single namespace so aren't shared across all applications. And finally, the community. Kubernetes and in general, the cloud native community is one of the strongest open source communities that I've ever seen. It is governed by the Cloud Native Computing Foundation, also known as the CNCF. Kubernetes also has great documentation. I find myself looking at Kubernetes docs on a regular basis and really appreciate how comprehensive they are. In terms of meetups and conferences, KubeCon or CloudNativeCon is the big conference that happens three times a year, once in Europe, once in Asia, and another in the United States. There are also regional Kubernetes community days that have occurred in different cities, as well as meetups that happen on a monthly basis in many cities. Check out meetup.com to see if there's one in your area. There's also a Slack group for Kubernetes that is very active. Once you join Slack, you will find different special interest groups or SIGs where you'll find the maintainers hang out and it's a great place to ask questions, you'll typically find me in the odd SIG room. In this section, I've covered some of the most compelling features in Kubernetes and why you'd want to pick Kubernetes for your org. Next up, let's talk about some of the other orchestrators out there.


Other implementations

-  Now that you know what Kubernetes is all about, it's time to discuss other platforms that are similar. We're in the software world, after all, and there are many ways to solve the same problem. Before we begin, I need to get something off my chest. Container orchestration is a very complicated topic. When containers first became a hot topic, a lot of folks, enterprises included, were building their own orchestration tools in house, because the landscape was very nascent at that time. Eventually, almost every company ended up moving to one of the solutions I'll talk about in this chapter. So in short, don't build your own orchestrator, you'll only regret it, and end up moving to something else anyways. The cloud data landscape of today is typically based around Kubernetes, but there are also other technologies and architectures that are popular. Mesos and Rancher are popular from a container perspective, and serverless technologies are also gaining popularity for application modernization. There are also Cloud-Specific Technologies, like Amazon EC2 Container Service or Google Anthos that are technologies built specifically for those ecosystems. But we won't spend too much time comparing them in this section. We talked about Kubernetes, so let's take a look at some of its competitors. Mesos is written in C++, with APIs in Java, C++ and Python. It has a distributed kernel where many machines act as one logical entity. The Marathon framework in Mesos is used to schedule and execute tasks, and finally, Mesos has a more complex architecture compared to other technologies like Dock or Swarm, for example. Typical users of Mesos are larger enterprises that require a lot of compute, or job- or task-oriented workloads. Mesos is often used by companies that have to perform big data jobs, it's driven by developers, rather than operations, but requires an ops team to manage the tool. Rancher is a full stack container management platform, it allows you to install and manage Kubernetes clusters easily, whether on premise or in the cloud. It was an early player in the container ecosystem, and had orchestration concepts, even before they became a hot topic. It has a great user interface and API to interact with clusters, and provides enterprise support for its tooling. I've seen a lot of folks use Rancher, from small teams to large enterprises. Folks that like to use Rancher really enjoy the user interface and APIs, and allows them to manage their infrastructure very effectively. This chart plots the number of hosts and containers versus the size of the development team. If you're running a lean shop, you should consider some of the solutions on the left. If you're a larger enterprise, you probably lean towards one of the more feature-rich solutions on the right. Another perspective on the container space is to look at yearly surveys. There are a few of these, from the CNCF Survey, to Datadog, to devops.com, et cetera. We'll look at the Cloud Native Computing Foundation Survey, from 2019, which states that: containers are the new normal with 84% use in production this year, and Kubernetes leading the charge in terms of container platforms, with 78% of the respondents saying that they're using Kubernetes in production already. Another trend we continue to see is that many enterprises have joined the CNCF, which backs Kubernetes. As a article in ARCHITECT states: At this point, literally every U.S.-based technology vendor or cloud provider that matters is a CNCF member and most have a clear and very active Kubernetes strategy. While Kubernetes is very popular, there is also a trend for companies to use serverless technologies. This might be something you'd consider if you're building all your infrastructure from scratch and you're running your infrastructure on a cloud provider. We've covered other competing technologies in this section. Join me in the next chapter to dive into the Kubernetes terminology and landscape.