Architecture of a Kubernetes cluster

-  Let's take a look at the entire Kubernetes Cluster Architecture. As you can tell, its got a lot of boxes, and even more arrows. Let's break this down and build it back up. First up, we have the Master Node. This guy is responsible for the overall management of the Kubernetes Cluster. Its got three components that take care of communication, scheduling, and controllers. These are the API Server, Scheduler, and Cluster Manager. The Kube API Server, as the name states, allows you to interact with the Kubernetes API. It's the front end of the Kubernetes control plane. Next up, we have the Scheduler. The Scheduler watches created Pods, who do not have a Node design yet, and designs the Pod to run on a specific Node. The Controller Manager runs controllers. These are background threads that run tasks in a cluster. The controller actually has a bunch of different roles, but that's all compiled into a single binary. The roles include, the Node Controller, who's responsible for the worker states, the Replication Controller, which is responsible for maintaining the correct number of Pods for the replicated controllers, the End-Point Controller, which joins services and Pods together. Service account and token controllers that handle access management. Finally, there's that CD, which is a simple distributed key value stored. Kubernetes uses etcd as its database and stores all cluster data here. Some of the information that might be stored, is job scheduling info, Pod details, stage information, etc. And, that's the Master Node. You interact with the Master Node using the Kubectl application, which is the command line interface for Kubernetes. Kubectl is also also called Kubectl, in some instances, but I'm going to say Kubectl for the rest of the course. Kubectl has a config file called a Kubeconfig. This file has server information, as well as authentication information to access the API Server. We wouldn't get anywhere without Worker Nodes, though. These Worker Nodes are the Nodes where your applications operate. The Worker Nodes communicate back with the Master Node. Communication to a Worker Node is handled by the Kubelet Process. It's an agent that communicates with the API Server to see if Pods have been designed to the Nodes. It executes Pod containers via the container engine. It mounts and runs Pod volume and secrets. And finally, is aware of Pod of Node states and responds back to the Master. It's safe to say that if the Kubelet isn't working correctly on the Worker Node, you're going to have issues. As I mentioned before, Kubernetes is an container orchestrator, so the expectation is that you have a container native platform running on your Worker Nodes. This is where Docker comes in and works together with Kubelet to run containers on the Node. You could use alternate container platforms, as well, like (mumbles), but not a lot of folks do this anymore. The next process I'll talk about is the Kube-proxy. This process is the Network Proxy and load balancer for the service, on a single Worker Node. It handles the network routing for TCP and UDP Packets, and performs connection forwarding. Alright, we're in homestretch. Having the Docker Demon allows you to run containers. Containers of an application are tightly coupled together in a Pod. By definition, a Pod is the smallest unit that can be scheduled as a deployment in Kubernetes. This group of containers share storage, Linux name space, IP addresses, amongst other things. They're also call located and share resources that are always scheduled together. Once Pods have been deployed, and are running, the Kubelet process communicates with the Pods to check on state and health, and the Kube-proxy routes any packets to the Pods from other resources that might be wanting to communicate with them. Worker Nodes can be exposed to the internet via load balancer. And, traffic coming into the Nodes is also handled by the Kube-proxy, which is how an End-user ends up talking to a Kubernetes application. That's the entire Kubernetes architecture. We'll go into more detail on these components in the coming section. But, from a high level, you know how everything works together.


Basic building blocks: Nodes and pods

-  In the last section, I covered the overall Kubernetes cluster architecture. In this section, we'll take a deeper look at some of the components and concepts that one uses on worker Nodes to build applications and Kubernetes. This is another way to look at the same architecture, but from a cluster perspective. From this perspective, the Master is responsible for managing the cluster. It coordinates all the activities in the cluster and communicates with the Nodes to keep Kubernetes and your applications running. The Node serves as a worker machine in the Kubernetes Cluster. One important thing to note is that this Node can be a physical computer or a virtual machine. The Node has the following requirements. Each Node must have a Kubelet running, container tooling, like Docker, a kube-proxy process running, and a process like Supervisord, so it can restart components. I'll go over more details of the specifics of these components in a later section. One thing to note is that if you're using Kubernetes in a production like setting, it's recommended that you have at least a three Node cluster. For this course, we will use Minikube, which is a tool run Kubernetes locally. It's a lightweight Kubernetes implementation that creates a virtual machine, on your local box, and deploys a simple cluster containing of one single Node. Your applications run on Nodes, so let's take a look at the most basic construct needed to build a Kubernetes app. This is called a Pod. In the Kubernetes model, a Pod is the simplest unit that you can interact with. You can create, deploy, and delete Pods, and it represents one running process in your cluster. A Pod contains the following things. Your Docker application container, storage resources, a unique network IP, and options that govern how the container should run. In some scenarios, you can have multiple docker containers running in a Pod, but a Pod represents one single unit of deployment, a single instance of an application in Kubernetes that's tightly coupled and shares resources. Pods are designed to be ephemeral, disposable entities. I never create Pods just by themselves in a production application. I only do that when I need to test whether the underlying containers actually work. Pods also don't self-heal. If a Pod dies, for some reason, it will not be rescheduled. Also, if a Pod is exited from a Node because of lack of resources, it will not be restarted on different healthier Nodes. There are higher level constructs to manage and add stability to Pods, called controllers. So pro-tip, don't use a Pod directly. Use a controller instead, like a deployment. Through its life-cycle, a Pod has the following states. Pending, which means that the Pod has been accepted by the Kubernete system, but a container has not been created yet. Running, where a Pod has been scheduled on a Node, and all of its containers are created, and at least one container is in a running state. Succeeded, which means that all the containers in the Pod have exited with an exit stat of zero, which indicates successful execution, and will not be restarted. A failed state, which means all the containers in the Pod have exited and at least one container has failed and returned a non-zero exit status. Or, my favorite, which is the CrashLoopBackOff. This is where a container fails to start, for some reason, and then Kubernetes tries over and over and over again to restart the Pod. Now that we've discussed two basic concepts, Nodes and Pods, in the next video, we'll build on these concepts and talk about deployments, replicacets, and services.



Deployments, jobs, and services

-  We've seen how pods are the basic building blocks in Kubernetes. As discussed in the previous video, we don't want to use them by themselves and want to use controllers instead. In this video, we'll go over what controllers are and how they help us. Before we dive into the details, it's good to understand what problems controllers actually help us solve. These are application reliability, where multiple instances of an application running prevent problems if one or more instance fails. Scaling. When your pods experience a high volume of requests, Kubernetes allows you to scale up your pods, allowing for a better user experience. And finally, load balancing, where having multiple versions of a pod running allow traffic to flow to different pods and doesn't overload one single pod or a node. I'll cover the following kinds of controllers, ReplicaSets, deployments, DaemonSets, jobs, and Services. First off, ReplicaSets. A ReplicaSet has one job, it ensures that the specified number of replicas for a pod are running at all times. If the number of pods is less than what the ReplicaSet expects, for example, when a pod might have crashed, the ReplicaSet controller will start up a new pod, however, you can't actually declare a ReplicaSet by itself. You'll need to use it within a deployment, so let's look at what that is. A Deployment Controller provides declarative updates for pods and ReplicaSets. This means that you can describe the desired state of a deployment in a YAML file and the Deployment Controller will align the actual state to match. Deployments can be defined to create new ReplicaSets or replace existing ones with new ones. Most applications are packages deployments, so chances are, you'll end up creating Deployments more than anything else. Essentially, a deployment manages a ReplicaSet which, in turn, manages a pod. The benefit of this architecture is that deployments can automatically support a role-back mechanism. A new ReplicaSet is created each time a new Deployment config is deployed, but it also keeps the old ReplicaSet. This allows you to easily roll back to the old state if something isn't quite working correctly. Deployment Controllers and objects are higher-level constructs that were introduced to solve specific issues. Pod management, where running a ReplicaSet allows us to deploy a number of pods and check their status as a single unit. Scaling a ReplicaSet, which scales out the pods and allows for the deployment to handle more traffic. Pod updates and role-backs, where the Deployment Controller allows updates to the PodTemplateSpec. This creates a new ReplicaSet and deploys a newer version of the pod. Also, if you don't like what you see in the newer version of the pod, just roll-back to the old ReplicaSet. Pause and resume. Sometimes we have larger changesets or multiple updates that need to happen to a deployment. In these scenarios, we can pause a deployment, make all the necessary updates, and then resume the deployment. Once the deployment is resumed, the new ReplicaSet will be started up and the deployment will update as expected. Please note that while a deployment is paused, it means that only updates are paused, but traffic will still get passed to the existing ReplicaSet as expected. And finally, status. Getting the deployment status is an easy way to check for the health of your pods and identify issues during a roll-out. We'll go deeper into how deployments work in the demo section of this course. You might have come across Replication Controllers if you search for Kubernetes controllers. This was an early implementation of ReplicaSets, but it has since been replaced by deployments and ReplicaSets, so in short, use deployments and ReplicaSets instead. Next up we have DaemonSets. DaemonSets ensure that all nodes run a copy of a specific pod. As nodes are added or removed from the cluster, a DaemonSet will add or remove the required pods. Deleting a DaemonSet will also clean up all the pods that it created. The typical use case for a DaemonSet is to run a single log aggregator or monitoring agent on a node. A job, as the name suggests, is basically a supervisor process for pods carrying out batch processes to completion. As the pod completes successfully, the job tracks information about the completion state of the pod. Jobs are used to run individual processes that need to run once and complete successfully. Typically, jobs are run as a cron job to run a specific process at a specific time and repeat at another time. You might use a cron job to run a nightly report or database backups, for example. A service provides network connectivity to one or more pods in your cluster. When you create a service, it's designed a unique IP address that never changes through the lifetime of the service. Pods are then configured to talk to the service and can rely on the service IP on any requests that might be sent to the pod. Services are a really important concept because they allow one set of pods to communicate with another set of pods in an easy way. It's a best practice to use a service when you're trying to get two deployments to talk to each other. That way, the pod in the first deployment always has an IP that they can communicate with regardless of whether the pod IPs in the second deployment changes. For example, when your front-end deployment needs to call a back-end deployment, you want to address the back-end with a service IP. Using the Backend Pod IP is a bad choice here because it can change over time and would wreak havoc for your application. A service provides an unchanging address so that the Frontend Pods can effectively talk to them at all times. There's a few kind of services that you can use. Internal services, where an IP is only reachable from within the cluster, this is the cluster IP in Kubernetes speak, then there are external services where services running web servers, or publicly accessible pods, are exposed through an external endpoint. These endpoints are available on each node through a specific port. This is called a NodePort in Kubernetes speak. And finally, we have the load balancer. This is for use cases when you want to expose your application to the public internet. It's only used when you are using Kubernetes in a cloud environment backed by a cloud provider such as AWS. I've covered a lot of basic concepts in this section. If you're a little fuzzy on some of this, I think that seeing it in action in our demo section will help clarity some of these concepts. Next up, we will learn about how to organize your applications with Labels, Selectors, and NameSpaces.


Labels, selectors, and namespaces

-  The last must know concept to build applications are labels, selectors, and namespaces. These constructs allow us to agitate and organize our applications so that when we have a lot of them, Kubernetes operators still understand what's going on holistically. Labels are key value pairs that are attached to objects like pods, services, and deployments. Labels are for us, the users of Kubernetes, to identify attributes for objects. Typically, labels are used to organize clusters in some meaningful way. They can be added at deployment time, or later on and changed at any time. Label keys are unique per object. Here are some examples of labels we might use. We might have a release label that has stable or canary deploys. We could have environment labels that specify the environment, such as dev, qa, production, et cetera, or perhaps tier labels to signify that something is a front end or a back end tier. As you can tell, labels are very specific to your use case, and they are built for users, so think about what your environment looks like, and how you want to organize your applications, and then get your label maker out. By themselves, labels aren't really that powerful. But when you add selectors, you introduce a very powerful feature. With labels and selectors, you can identify a specific set of objects. There are two kinds of selectors, equality-based and set-based. Equality-based selectors include the equals and not equals, where the equals represents equality, where two labels or values of labels should be equal. Not equal represents inequality. This means that the values of the labels should not be equal. Next, we have set-based selectors that include IN, NOTIN, and EXISTS operators. The IN operator specifies that the value should be in a set of defined values. The NOTIN operators specified that the value should not be in a set of defined values. And finally, the EXISTS operator is used to determine whether a label exists or not. Labels and label selectors are typically used with a kubectl command to list and filter objects. We'll look at examples of how to use labels and selectors later on in the demo section of the course. And finally, we have namespaces. Unlike labels and selectors, the namespace concept is a feature of Kubernetes that allows you to have multiple virtual clusters backed by the same physical cluster. Namespaces are a great concept to use for large enterprises where there are many users and teams and you want to give access to different teams but at the same time have a rough idea of who owns what in the Kubernetes environment. For example, if you have a big e commerce company, you might have a namespace for your catalog team, card team and order status team to run their different applications. It's also a great way to divide cluster resources between multiple users and this can be done using resource quotas. Namespaces provide scope for names. Names of resources, like deployments and pods, must be unique within the namespace, but not necessarily across separate namespaces. So in our example, the catalog team and the card team can have an application name authentication in their own namespaces. When you launch Kubernetes, there is a default namespace where all our objects get placed, but you are allowed to create new namespaces as and when you wish. Also you'll notice that when you install newer application Kubernetes, they'll typically install in a brand new namespace, so that they don't interfere with your existing cluster and cause confusion. In this section, we've gone over ways to organize your applications. Join me in the next section to go cover the Kubelet and Kube-proxy topics.

Kubelet and kube proxy

-  We had talked about kubelet and kube-proxy components when we discussed the overall architecture earlier. Let's dig into these a bit. The kubelet is the Kubernetes node agent that runs on each node. It has many roles. It communicates with the API server to see if pods have been assigned to the nodes. It executes the pod containers via the container engine. It mounts and runs pod volumes and secrets. It executes health checks and is aware of pod and node status and reports that back to the API server. The kubelet works in terms of Podspec, which is just a YAML file that describes the pod. The kubelet takes a set of Podspecs that are provided by the kube-apiserver and ensures that the containers described in those Podscpecs are running and healthy. It's important to note that the kubelet only manages containers that were created by the API server, and not any other containers that might be running on the node. We can also manage the kubelet without an API server by using HTP endpoint or a file, but that's beyond the scope of this course. The network proxy is called the kube-proxy. This is another process that runs on all the worker nodes. The kube-proxy reflects services that are defined in the API on each node and can do simple network stream or round-robin forwarding across a set of backends. Service cluster IPs and ports are currently found through Docker-link compatible environment variables specifying ports opened by the service proxy. The kube-proxy has three modes: the user space mode, Iptables mode, and ipvs mode, which is an alpha feature in Kuberetes 1.8. The user space is the most common mode, and the one we will use in this course. If you're really interested in the gory networking details, take a look at the handout for more information on those. These modes are important when it comes to using services. Services are defined against the API server. The kube-proxy watches the API server for addition and removal of services. For each new service, kube-proxy opens a randomly chosen port on the local node. Any connections made to that port are proxied to one of the corresponding back-end pods. Join me in the next section, where we'll get mini kube set up and run some demos.